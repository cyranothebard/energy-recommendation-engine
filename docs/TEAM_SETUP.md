# Energy Recommendation Engine - Team Setup Guide

## ğŸš€ Quick Start (5 minutes)

### 1. Repository Access
```bash
git clone https://github.com/cyranothebard/energy-recommendation-engine.git
cd energy-recommendation-engine
```

### 2. AWS Configuration
You should have received AWS credentials. Configure them:
```bash
aws configure
# Use the credentials provided in team communication
```

### 3. Python Environment Setup
```bash
# Create conda environment
conda create -n energy-rec python=3.9
conda activate energy-rec

# Install requirements
pip install pandas numpy boto3 matplotlib seaborn jupyter
```

### 4. Verify Setup
```bash
# Test AWS access
aws s3 ls s3://energy-recommendation-project-246773437083/

# Should show: raw-data/ folder
```

## ğŸ“ Project Structure

```
energy-recommendation-engine/
â”œâ”€â”€ src/energy_recommender/           # ğŸ†• PRODUCTION CODE
â”‚   â”œâ”€â”€ features/engineering.py       # Feature engineering pipeline
â”‚   â”œâ”€â”€ models/compliance.py          # Compliance prediction
â”‚   â”œâ”€â”€ models/optimization.py        # Portfolio optimization
â”‚   â””â”€â”€ pipeline.py                   # End-to-end orchestration
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ exploratory/                  # Research notebooks (Brandon's work)
â”‚   â””â”€â”€ clean/                        # ğŸ†• TEAM DEMOS (coming soon)
â”œâ”€â”€ data/sample/                      # Sample data for development
â”œâ”€â”€ config/                           # Configuration files
â””â”€â”€ tests/                           # Unit tests (future)
```

## ğŸ¯ Team Roles & Next Steps

### ğŸ‘¨â€ğŸ’» Brandon (Technical Lead)
- **Status**: Pipeline validation complete âœ…
- **Next**: ML model training + distributed processing
- **Focus**: Spark implementation, model optimization

### ğŸ‘©â€ğŸ’» Dashboard Developer
- **Goal**: Interactive recommendation visualization
- **Data Source**: Use `pipeline.py` outputs for sample data
- **Tech Stack**: Plotly Dash, Bootstrap
- **Start Here**: Create `notebooks/clean/dashboard_demo.ipynb`

### ğŸ“Š Documentation Lead  
- **Goal**: Evaluation framework + presentation
- **Focus**: A/B testing metrics, performance analysis
- **Start Here**: Create `notebooks/clean/evaluation_analysis.ipynb`

## ğŸ”§ Development Workflow

### Working with the Pipeline
```python
# Import the production modules
from src.energy_recommender.pipeline import run_end_to_end_pipeline
from src.energy_recommender.features.engineering import engineer_building_features_comprehensive

# Load sample data (provided)
metadata_df = pd.read_csv('data/sample/sample_buildings.csv')

# Run complete pipeline
results = run_end_to_end_pipeline(metadata_df)

# Access outputs
building_features = results['building_features']
portfolio = results['portfolio_results']
```

### Sample Data Available
- **Location**: `s3://energy-recommendation-project-246773437083/raw-data/`
- **Sample Buildings**: 50 MA buildings with full timeseries
- **Metadata**: 8,111 buildings with characteristics
- **Format**: CSV files, ready to use

## ğŸ“‹ Key Accomplishments (Context)

### âœ… Validated Pipeline Results
- **Feature Engineering**: 13 building types, 34 HVAC systems
- **Compliance Modeling**: 36.3% average compliance (realistic)
- **Portfolio Optimization**: 5.4% grid reduction (industry benchmark)
- **Performance**: <30 seconds, <50MB memory

### âœ… AWS Infrastructure  
- **Budget Controls**: $100/month with alerts
- **Data Access**: All team members have S3 access
- **Scalability**: Ready for SageMaker when needed

## ğŸš¨ Important Notes

### Data Handling
- **DO NOT** download full dataset locally (14.5 GiB)
- **USE** sample data in `data/sample/` for development
- **AWS costs** are monitored - avoid unnecessary compute

### Code Organization
- **Research code**: `notebooks/exploratory/` (Brandon's domain)
- **Production code**: `src/energy_recommender/` (shared team code)
- **Clean demos**: `notebooks/clean/` (for team collaboration)

### Git Workflow
- **Main branch**: Always deployable
- **Feature branches**: For individual development
- **Pull requests**: For code review

## ğŸ“ Getting Help

### Immediate Issues
1. **AWS Access Problems**: Check credentials with `aws sts get-caller-identity`
2. **Python Environment**: Ensure you're in the `energy-rec` conda environment
3. **Data Access**: Verify S3 permissions with the test command above

### Team Communication
- **Technical Questions**: Tag Brandon in team chat
- **AWS Issues**: Check AWS console budget dashboard
- **Git Problems**: Use GitHub issues or team chat

## ğŸ¯ Success Criteria

### Week 1 Goals
- **Dashboard Developer**: Basic visualization of portfolio results
- **Documentation Lead**: Evaluation metrics framework
- **Technical Lead**: ML model training pipeline

### Integration Points
- **Data Interface**: All modules use standard DataFrame formats
- **API Design**: Clean function signatures for easy integration
- **Performance**: All components process sample data in <10 seconds

---

## ğŸš€ Ready to Start?

1. **Complete setup steps above** (should take 5-10 minutes)
2. **Run the pipeline test** to verify everything works
3. **Check team chat** for role-specific guidance
4. **Begin development** in your assigned area

The pipeline is validated and ready - let's build something great! ğŸ”¥